# -*- coding: utf-8 -*-
"""Data Cleaning & EDA PKL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JN-gyUINEFtwc005TkatEzQC9VfQwp6d

# **Data Cleaning**

## **Import Package**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Import additional files with statistical functions
import sys
import os

"""### **Data**"""

data = pd.read_excel('DATA AR JANUARI - MEI 2023 - FIX - Copy.xlsx')
data.head()

"""## **Feature Engineering**"""

#drop rows that contain specific 'value' in 'column_name'
data2 = data[(data.Bandwidth != "Units") & (data.Layanan != "Penambahan IPv4 Publik Internet Corporate")]
data3 = data2[data2.AL_Status == "Active"]
data4 = data3[data3.SBU_Ter == "SBU REG JAWA BAGIAN TIMUR"]
data5 = data4[data4.AR_Type != "Trial"]
data5.head()

data6 = data5.drop(['Nomor', 'Segment','Cust_Name', 'Address_Terminating', 'SBU_Ter', 'AL_Status'], axis=1)
data6.head()

data6['AR_Type'].value_counts()

#data7['AR_Type'].value_counts()

data6['Bidang Baku'].value_counts()

#data7['Bidang Baku'].value_counts()

data['Tipe'].value_counts()

#data7['Tipe'].value_counts()

data5['Kabupaten/Kota'].value_counts()

#data7['Kabupaten/Kota'].value_counts()

data6['Layanan'].value_counts()

data6['Bandwidth'].value_counts()

data6['Wilayah'].value_counts()

#data7['Wilayah'].value_counts()

"""## **Preprocessing**"""

data6.info() #informasi dataset

# Cek missing value
data6.isnull().sum()

# example of a ordinal encoding
from numpy import asarray
from sklearn.preprocessing import OrdinalEncoder
# Select the columns for ordinal encoding
data7 = data6

# Define ordinal encoder
encoder = OrdinalEncoder()

# Perform ordinal encoding
data7[["AR_Type","Bidang Baku","Tipe","Layanan","Kabupaten/Kota","Wilayah"]] = encoder.fit_transform(data7[["AR_Type","Bidang Baku","Tipe","Layanan","Kabupaten/Kota","Wilayah"]])

# Print the encoded dataset
data7.head()

data7["Layanan"].value_counts()

data7.info() #informasi dataset

# converting  from float to int
data7["AR_Type"] = data7["AR_Type"].astype(int)
data7["Bidang Baku"] = data7["Bidang Baku"].astype(int)
data7["Tipe"] = data7["Tipe"].astype(int)
data7["Layanan"] = data7["Layanan"].astype(int)
data7["Bandwidth"] = data7["AR_Type"].astype(float)
data7["Kabupaten/Kota"] = data7["Kabupaten/Kota"].astype(int)
data7["Wilayah"] = data7["Wilayah"].astype(int)

data7.info() #informasi dataset

"""# **Exploratory Data Analysis (EDA)**

## **Descriptive Analysis**
"""

# Summary statistics
summary_stats = data7.describe()
print(summary_stats)

"""## **Plot Histogram**"""

# Density plot
fig, ax = plt.subplots()

sns.kdeplot(data7[data7["AR_Type"]=="Change Tariff"]["Biaya_Sewa"], fill=True, color="blue", label="Change Tariff", ax=ax)
sns.kdeplot(data7[data7["AR_Type"]=="Downgrade"]["Biaya_Sewa"], fill=True, color="green", label="Downgrade", ax=ax)
sns.kdeplot(data7[data7["AR_Type"]=="New"]["Biaya_Sewa"], fill=True, color="red", label="New", ax=ax)
sns.kdeplot(data7[data7["AR_Type"]=="Relocation"]["Biaya_Sewa"], fill=True, color="yellow", label="Relocation", ax=ax)
sns.kdeplot(data7[data7["AR_Type"]=="Upgrade"]["Biaya_Sewa"], fill=True, color="pink", label="Upgrade", ax=ax)
sns.kdeplot(data7[data7["AR_Type"]=="Trial"]["Biaya_Sewa"], fill=True, color="purple", label="Trial", ax=ax)

ax.set_xlabel("Biaya_Sewa")
ax.set_ylabel("Density")

plt.show()

plt.show()

# Histogram of a Biaya_Sewa
plt.hist(data6['Biaya_Sewa'], bins=10)
plt.xlabel('Biaya_Sewa')
plt.ylabel('Frequency')
plt.title('Histogram of Biaya_Sewa')
plt.show()

# Select the categorical variables
categorical_vars = ['AR_Type', 'Bidang Baku', 'Layanan', 'Bandwidth', 'Kabupaten/Kota', 'Wilayah']

# Create subplots for each categorical variable
fig, axes = plt.subplots(len(categorical_vars), 1, figsize=(8, 6 * len(categorical_vars)))
for i, var in enumerate(categorical_vars):
    sns.countplot(x=var, data=data7, ax=axes[i])
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'Distribution of {var}')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Grouped bar chart
fig, ax = plt.subplots()

sns.catplot("Layanan", hue="AR_Type", data=data7, kind="object", ax=ax)

plt.close(2) # catplot creates an extra figure we don't need

ax.legend(title="Passenger Class")
#ax.set_xticklabels(["did not survive", "survived"])
ax.set_xlabel("")

fig.suptitle("Passenger Class vs. Survival for Titanic Passengers");

# Correlation matrix
correlation_matrix = data7.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Box plot of a Biaya_Sewa with potential outliers
sns.boxplot(x='Biaya_Sewa', data=data)
plt.xlabel('Biaya_Sewa')
plt.title('Box Plot of Biaya_Sewa')
plt.show()

"""## **Check Outliers**"""

# Fill NaN values with 0
data7 = data7.fillna(0)

# Check the updated dataset
print(data7.info())

import pandas as pd
import numpy as np
from scipy import stats

# Select the numerical variables for outlier detection
numerical_vars = ['AR_Type', 'Bidang Baku', 'Layanan', 'Bandwidth', 'Biaya_Sewa', 'Kabupaten/Kota', 'Wilayah']

# Detect outliers using z-score method
outliers = []
for var in numerical_vars:
    z_scores = stats.zscore(data7[var])
    threshold = 3  # Adjust this threshold as needed
    var_outliers = data7[np.abs(z_scores) > threshold]
    outliers.append(var_outliers)

# Concatenate the outliers for all variables
outliers = pd.concat(outliers)

# Print the outliers
print("Outliers:")
print(outliers)

"""## **Chi-Square Tests**"""

data7.columns

"""**AR_Type**"""

cross_tab1 = pd.crosstab(data7['AR_Type'], data7['Layanan'])
cross_tab1

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab1)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""**Bidang Baku**"""

cross_tab2 = pd.crosstab(data7['Bidang Baku'], data7['Layanan'])
cross_tab2

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab2)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""**Tipe**"""

cross_tab2 = pd.crosstab(data7['Tipe'], data7['Layanan'])
cross_tab2

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab2)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""**Bandwidth**"""

cross_tab3 = pd.crosstab(data7['Bandwidth'], data7['Layanan'])
cross_tab3

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab3)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""**Biaya_Sewa**"""

cross_tab4 = pd.crosstab(data7['Biaya_Sewa'], data7['Layanan'])
cross_tab4

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab4)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""**Kabupaten/Kota**"""

cross_tab5 = pd.crosstab(data7['Kabupaten/Kota'], data7['Layanan'])
cross_tab5

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab5)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""**Wilayah**"""

cross_tab6 = pd.crosstab(data7['Wilayah'], data7['Layanan'])
cross_tab6

chi_sq, p_val, dof, con_table = stats.chi2_contingency(cross_tab6)
print(f'chi-squared = {chi_sq}')
print(f'p-value= {p_val}')
print(f'degrees of freedom = {dof}')

"""## **Save the DataFrame**"""



"""#**Modeling**

## Split Data
"""

import pandas as pd
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression as LR
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression #digunakan untuk membuat model multinomial logistic regression
from sklearn.naive_bayes import GaussianNB #digunakan untuk membuat model naive bayes
from sklearn.tree import DecisionTreeClassifier #digunakan untuk membuat model decision tree
from sklearn.ensemble import RandomForestClassifier #digunakan untuk membuat model random forest
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

# Load the data into a DataFrame
df = pd.DataFrame(data6)

# Split the data into training and testing sets
X = df.drop('Layanan', axis=1)
y = df['Layanan']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

"""## Data Imbalance"""

pd.Series(y_train).value_counts(normalize=True)

"""It is clear that train set is imbalanced. I will build first model with this data and later on try to balance it using SMOTE.

## Baseline Model - Multinomial Logistic Regression
"""

cat_cols = X_train.select_dtypes(include='object').columns
indices = []
for col in cat_cols:
    indices.append(X_train.columns.get_loc(col))
indices

transformer = ColumnTransformer(transformers=[('categorical',
                                               OneHotEncoder(handle_unknown = 'ignore'), indices)],
                                remainder = 'passthrough')

# Perform multinomial logistic regression
lr = LogisticRegression(multi_class='multinomial', solver='newton-cg', max_iter=20000)

# Create the pipeline including the transformer, scaler, and logistic regression model
base_model_pipe = make_pipeline(transformer, StandardScaler(with_mean=False), lr)

# Fit the pipeline on the training data
base_model_pipe.fit(X_train, y_train)

# Make predictions on the test set
y_test_pred = base_model_pipe.predict(X_test)
y_train_pred = base_model_pipe.predict(X_train)

# Calculate the accuracy of the model
train_accuracy = accuracy_score(y_train, y_train_pred) * 100
test_accuracy = accuracy_score(y_test, y_test_pred) * 100
print("Train Set Accuracy: {:.2f}%".format(train_accuracy))
print("Test Set Accuracy: {:.2f}%".format(test_accuracy))

# Confusion matrix and classification report
print("\nConfusion Matrix:\n{}".format(confusion_matrix(y_test, y_test_pred)))
print("\nClassification Report:\n{}".format(classification_report(y_test, y_test_pred)))

residuals = y_train == y_train_pred

print('Number of values correctly predicted:')
print(pd.Series(residuals).value_counts())

residuals = y_test == y_test_pred

print('Number of values correctly predicted:')
print(pd.Series(residuals).value_counts())

"""## Multinomial Logistic Regression with Upsampled Data"""

print(X.columns)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Calculate the minimum number of samples (n_samples) among the 199 classes
n_samples = min([len(X_train[y_train == i]) for i in range(199)])

# Choose a value for the n_neighbors parameter
n_neighbors = 5  # Adjust this value as needed

# Apply SMOTE if n_neighbors <= n_samples, otherwise apply RandomOverSampler
if n_neighbors <= n_samples:
    oversampler = SMOTE(sampling_strategy='not majority', k_neighbors=n_neighbors)
else:
    oversampler = RandomOverSampler(sampling_strategy='not majority')

# Define the preprocessing steps for different types of features
preprocessor = ColumnTransformer([
    ('numeric', StandardScaler(), numerical_vars),
    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_vars)
])

# Apply oversampling to the training data
X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)

# Apply preprocessing steps separately
X_train_preprocessed = preprocessor.fit_transform(X_train_oversampled)
X_test_preprocessed = preprocessor.transform(X_test)

# Create the logistic regression model
lr = LogisticRegression(multi_class='multinomial', solver='newton-cg', max_iter=20000)

# Fit the logistic regression model on the preprocessed training data
lr.fit(X_train_preprocessed, y_train_oversampled)

# Make predictions on the test set
y_test_pred = lr.predict(X_test_preprocessed)
y_train_pred = lr.predict(X_train_preprocessed)

# Calculate the accuracy of the model
train_accuracy = accuracy_score(y_train_oversampled, y_train_pred) * 100
test_accuracy = accuracy_score(y_test, y_test_pred) * 100
print("Train Set Accuracy: {:.2f}%".format(train_accuracy))
print("Test Set Accuracy: {:.2f}%".format(test_accuracy))

# Confusion matrix and classification report
print("\nConfusion Matrix:\n{}".format(confusion_matrix(y_test, y_test_pred)))
print("\nClassification Report:\n{}".format(classification_report(y_test, y_test_pred)))

residuals = y_train == y_train_pred

print('Number of values correctly predicted:')
print(pd.Series(residuals).value_counts())

residuals = y_test == y_test_pred

print('Number of values correctly predicted:')
print(pd.Series(residuals).value_counts())

# # Save Model
# # Import the joblib package
# import joblib
# # Give the name to the saved model
# filename = 'lr_model.pkl'
# # Save the model (lr - pipeline of the final model)
# joblib.dump(lr, filename)

# save the model to disk
import pickle
filename = 'lr2_model.sav'
pickle.dump(lr, open(filename, 'wb'))

"""## Naive Bayes

"""

gnb = GaussianNB()
gnb.fit(X_train,y_train)

y_test_pred2 = gnb.predict(X_test)
y_train_pred2=gnb.predict(X_train)

print("Train Set Accuracy:"+str(accuracy_score(y_train_pred2,y_train)*100))
print("Test Set Accuracy:"+str(accuracy_score(y_test_pred2,y_test)*100))

#Confusion matrix and classification report
print("\nConfusion Matrix:\n%s"%confusion_matrix(y_test_pred2,y_test))
print("\nClassification Report:\n%s"%classification_report(y_test_pred2,y_test))

# Save Model
# Import the joblib package
import joblib
# Give the name to the saved model
filename = 'gnb_model.pkl'
# Save the model (lr - pipeline of the final model)
joblib.dump(gnb, filename)

"""## Decision Tree

"""

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

y_test_pred3 = dtc.predict(X_test)
y_train_pred3=dtc.predict(X_train)


print("Train Set Accuracy:"+str(accuracy_score(y_train_pred3,y_train)*100))
print("Test Set Accuracy:"+str(accuracy_score(y_test_pred3,y_test)*100))

#Confusion matrix and classification report
print("\nConfusion Matrix:\n%s"%confusion_matrix(y_test_pred3,y_test))
print("\nClassification Report:\n%s"%classification_report(y_test_pred3,y_test))

# Save Model
# Import the joblib package
import joblib
# Give the name to the saved model
filename = 'dtc_model.pkl'
# Save the model (lr - pipeline of the final model)
joblib.dump(dtc, filename)

"""## Random Forest"""

rf = RandomForestClassifier(n_estimators=100, random_state=123)
rf.fit(X_train, y_train)

y_test_pred4 = rf.predict(X_test)
y_train_pred4=rf.predict(X_train)


print("Train Set Accuracy:"+str(accuracy_score(y_train_pred4,y_train)*100))
print("Test Set Accuracy:"+str(accuracy_score(y_test_pred4,y_test)*100))

#Confusion matrix and classification report
print("\nConfusion Matrix:\n%s"%confusion_matrix(y_test_pred4,y_test))
print("\nClassification Report:\n%s"%classification_report(y_test_pred4,y_test))

# Save Model
# Import the joblib package
import joblib
# Give the name to the saved model
filename = 'rf_model.pkl'
# Save the model (lr - pipeline of the final model)
joblib.dump(rf, filename)

"""## Support Vector Machine"""

# from sklearn.svm import SVC

# svm = SVC(kernel='linear', random_state=42)
# svm.fit(X_train, y_train)

# y_test_pred5 = svm.predict(X_test)
# y_train_pred5=svm.predict(X_train)


# print("Train Set Accuracy:"+str(accuracy_score(y_train_pred5,y_train)*100))
# print("Test Set Accuracy:"+str(accuracy_score(y_test_pred5,y_test)*100))

# #Confusion matrix and classification report
# print("\nConfusion Matrix:\n%s"%confusion_matrix(y_test_pred5,y_test))
# print("\nClassification Report:\n%s"%classification_report(y_test_pred5,y_test))

"""## Evaluation"""

# from sklearn.metrics import roc_curve, roc_auc_score


# y_pred_proba = lr.predict_proba(np.array(X_test))[:,1]

# fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# sns.set()

# plt.plot(fpr, tpr)

# plt.plot(fpr, fpr, linestyle = '--', color = 'k')

# plt.xlabel('False positive rate')

# plt.ylabel('True positive rate')

# AUROC = np.round(roc_auc_score(y_test, y_pred_proba), 2)

# plt.title(f'Logistic Regression Model ROC curve; AUROC: {AUROC}');

# plt.show()

# # Importing all necessary libraries
# from sklearn.metrics import roc_curve, auc

# class_probabilities = lr.predict_proba(X_test)
# preds = class_probabilities[:, 1]

# fpr, tpr, threshold = roc_curve(y_test, preds)
# roc_auc = auc(fpr, tpr)

# # Printing AUC
# print(f"AUC for our classifier is: {roc_auc}")

# # Plotting the ROC
# plt.title('Receiver Operating Characteristic')
# plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
# plt.legend(loc = 'lower right')
# plt.plot([0, 1], [0, 1],'r--')
# plt.xlim([0, 1])
# plt.ylim([0, 1])
# plt.ylabel('True Positive Rate')
# plt.xlabel('False Positive Rate')
# plt.show()

# # Perform cross-validation
# cv_scores = cross_val_score(lr, X, y, cv=5)

# # Print the cross-validation scores
# print('Cross-Validation Scores:', cv_scores)
# print('Average Accuracy:', cv_scores.mean())